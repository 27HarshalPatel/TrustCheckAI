# ğŸš€ TrustCheckAI â€” Bias and Compliance Detaction Platform

<p align="center">
  <img src="./TrustCheckAI-demo.gif" width="650">
</p>

TrustCheckAI is an end-to-end bias and compliance auditing, explainability, and model-monitoring platform designed to evaluate bias, mitigate discrimination, explain model decisions, and continuously monitor deployed machine learning systems using Prometheus & Grafana.

It provides real-time dashboards, fairness metrics, model explainability (LIME), drift detection, automated PDF reporting, and user feedback collection â€” all wrapped in a modern Streamlit UI and containerized for seamless deployment.

---

## ğŸ“‘ Table of Contents
- âœ¨ Features
- ğŸ“š Project Structure
- ğŸ§° Technical Stack
- ğŸ“š Supported Datasets
- ğŸ— System Architecture
- ğŸ” Compliance, Fairness & Security
- ğŸ¨ Human-Centered Design (HCI)
- âš™ï¸ Installation & Setup
- ğŸ§ª Usage Workflow
- ğŸ“Š Prometheus Metrics & Grafana Dashboards
- ğŸ“‰ Drift Detection
- ğŸ“˜ PDF Report Generation
- ğŸ¥ Demonstration
- ğŸ›£ Roadmap
- ğŸ“„ Citations
- ğŸ¤ Acknowledgements

---

## âœ¨ Features

### ğŸŸ£ Bias Detection (AIF360)
- Statistical Parity Difference  
- Disparate Impact    

### ğŸ§  Explainability (XAI)
- **LIME** â€“ local explanations per prediction  

### ğŸ“‰ Drift Detection
- Kolmogorovâ€“Smirnov (KS) Test  

### âš™ Model Training & Evaluation
- Logistic Regression  
- Random Forest  
- 5-fold cross-validation  

### ğŸ” Real-Time Monitoring & Alerting
- Prometheus metric exporter  
- Grafana dashboards  
- Automated Slack alerts for accuracy/fairness drift  

### ğŸ“„ Automatic PDF Reporting
- Full bias report  
- Model performance summary  

### ğŸ–¥ Modern Streamlit UI
- Clean, intuitive layout  
- File upload, analysis, visualization  
- User feedback

### ğŸ³ Fully Containerized
- Streamlit  
- Prometheus  
- Grafana  
- Docker Compose orchestration  

---

## ğŸ“š Project Structure

```text
TrustCheckAI/
â”œâ”€â”€ .ipynb_checkpoints/        # Auto-generated Jupyter checkpoints
â”œâ”€â”€ __pycache__/               # Python bytecode cache
â”œâ”€â”€ .DS_Store                  # macOS system metadata
â”œâ”€â”€ Dockerfile                 # Docker build instructions
â”œâ”€â”€ Final Report.pdf           # Final Project Report Template 
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ TrustCheckAI-Demo.mp4      # Full application demo video
â”œâ”€â”€ TrustCheckAI-demo.gif      # GIF preview for README
â”œâ”€â”€ compas-scores-two-years.csv # COMPAS dataset for fairness analysis
â”œâ”€â”€ docker-compose.yml         # Multi-service orchestration (Streamlit + Prometheus + Grafana)
â”œâ”€â”€ feedback.log               # Logs for user feedback & events
â”œâ”€â”€ prometheus.yml             # Prometheus scraping config
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ streamlit_app.py           # Main Streamlit application
```
---

## ğŸ§° Technical Stack

### ML & Fairness
- Python 3.9+  
- Scikit-learn  
- AIF360  
- LIME  

### Monitoring & Observability
- Prometheus  
- Grafana  

### Frontend
- Streamlit  

### DevOps
- Docker  
- Docker Compose  
- GitHub  

---

## ğŸ“š Supported Datasets

### COMPAS â€“ Criminal Justice  
### User Uploaded Structured Dataset

Each dataset includes at least one **protected attribute** such as race, gender, or age that is used for fairness auditing.
---

## ğŸ— System Architecture

The high-level architecture of TrustCheckAI is shown below:

```txt
                   +---------------------------+
                   |        User (UI)         |
                   |  â€¢ Upload CSV dataset    |
                   |  â€¢ Configure analysis    |
                   +-------------+------------+
                                 |
                                 v
                     +-----------+-----------+
                     |  Streamlit Application |
                     |  â€¢ Orchestration       |
                     |  â€¢ UX & controls       |
                     +-----------+------------+
                                 |
         +-----------------------+------------------------+
         |                        |                        |
         v                        v                        v
+----------------+     +-----------------------+   +---------------------+
| Preprocessing  |     | Bias & Fairness       |   | Model Training &    |
| & Validation   |---->| Analysis (AIF360)     |-->| Evaluation (SKL)    |
| â€¢ Cleaning     |     | â€¢ Metrics & thresholds|   | â€¢ LR / RF           |
+----------------+     +-----------------------+   +---------------------+
                                                             |
                                                             v
                                              +-----------------------------+
                                              | Explainability (LIME)       |
                                              +-----------------------------+
                                                             |
                                                             v
                                              +-----------------------------+
                                              | Drift Detection (KS)        |
                                              +-----------------------------+
                                                             |
                                                             v
                                           +-----------------+-----------------+
                                           | Prometheus Metrics Exporter      |
                                           | â€¢ upload_counter, accuracy_gauge |
                                           +-----------------+-----------------+
                                                             |
                                                             v
                                     +------------------------+---------------------+
                                     |          Grafana Dashboards & Alerts        |
                                     |  â€¢ Accuracy / fairness panels               |
                                     |  â€¢ Slack / email alerts                     |
                                     +----------------------------------------------+
```

**Component summary:**  
- **Streamlit App** â€“ central controller for data upload, analysis steps, and visualization.  
- **AIF360 Module** â€“ computes fairness metrics and applies mitigation algorithms.  
- **Model Training** â€“ trains ML models and logs metrics.  
- **XAI Module** â€“ generates LIME explanations for transparency.  
- **Drift Detection** â€“ monitors changes in data and predictions over time.  
- **Prometheus & Grafana** â€“ collect, visualize, and alert on key metrics.

---

## ğŸ§© Protected Attribute

In TrustCheckAI, the **protected attribute** is a sensitive feature such as **race, gender, age, or ethnicity** that represents groups we want to **protect from unfair treatment**.

Why it is important:

- ğŸ“ **Fairness metrics are defined with respect to protected groups.**  
  Measures like Statistical Parity Difference, Disparate Impact, and Equal Opportunity compare outcomes between protected and nonâ€‘protected groups. Without a protected attribute, these metrics cannot be computed.

- ğŸ§ª **Bias detection requires group-wise comparison.**  
  By conditioning on the protected attribute, TrustCheckAI can reveal whether the model treats one group systematically worse than another (e.g., lower approval rates or higher false-positive rates).

- ğŸ›¡ **Used for auditing, not for discrimination.**  
  In a responsible workflow, the protected attribute is often **excluded from the model features used for prediction**, but **retained in the evaluation pipeline** so that fairness can be audited postâ€‘hoc.

- ğŸ“œ **Regulatory and ethical compliance.**  
  Many regulations (EEOC, GDPR â€œspecial categoriesâ€, antiâ€‘discrimination laws) explicitly refer to protected characteristics. Correctly identifying and handling the protected attribute is essential for demonstrating compliance.

TrustCheckAI makes the protected attribute explicit in the UI and in the generated reports so that stakeholders clearly understand **which groups are being evaluated for fairness** and how mitigation affects them.

---

## ğŸ” Compliance, Fairness & Security
- Regulatory alignment (EEOC, Justice fairness)  
- Differential privacy  
- Ethical AI lifecycle tracking  
- Secure isolated containers  

---

## ğŸ¨ HCI Principles
- Accessible charts  
- Colorblind-safe design  
- Clear fairness/performance separation  
- Prototyped user flows  

---

## âš™ï¸ Installation & Setup

```bash
git clone https://github.com/27HarshalPatel/TrustCheckAI.git
cd TrustCheckAI
docker-compose up --build
```

Access:
- Streamlit â†’ http://localhost:8501  
- Prometheus â†’ http://localhost:9090  
- Grafana â†’ http://localhost:3000  

---

## ğŸ§ª Usage Workflow
1. Upload CSV
2. Select "Protected" Attribute
3. Select "Target" Variable
4. Run "Analyze Dataset" 
5. View the Bias and Compliance Check Result along with Accuracy in Predicting the Results
6. View LIME Analyses
7. Generate PDF  
8. Monitor in Grafana  
9. Receive alerts if Accuracy falls below 70%

---

## ğŸ“Š Prometheus Metrics
- upload_counter  
- analysis_counter  
- accuracy_gauge  
- feedback_ratings_counter  
- feedback_comments_counter  

---

## ğŸ“‰ Drift Detection
- KS Test 

---

## ğŸ“˜ PDF Report Generation
Includes fairness metrics and performance summary.

---

## ğŸ¥ Demonstration

<p align="center">
  <img src="./TrustCheckAI-demo.gif" width="700">
</p>

---

## ğŸ›£ Roadmap
- Fairlearn integration  
- Kubernetes deployment  
- Extended fairness metrics  

---

## ğŸ“„ Citations
- IBM AIF360  
- COMPAS Dataset  

---

## ğŸ¤ Acknowledgements
- University of Florida  
- HiPerGator Computing  
- Open-source community  
